{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pdf converter and summarizer working togather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "#install pdfminer first\n",
    "# PDF to Text Converter function--------------\n",
    "from pdfminer.pdfinterp import PDFResourceManager,PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import BytesIO\n",
    "def pdf_to_text(path):\n",
    "    manager = PDFResourceManager()\n",
    "    retstr = BytesIO()\n",
    "    layout = LAParams(all_texts=True)\n",
    "    device = TextConverter(manager, retstr, laparams=layout)\n",
    "    filepath = open(path, 'rb')#.read()\n",
    "    interpreter = PDFPageInterpreter(manager, device)\n",
    "    \n",
    "    for page in PDFPage.get_pages(filepath, check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "        text = retstr.getvalue()\n",
    "    filepath.close()\n",
    "    device.close()\n",
    "    retstr.close()\n",
    "    return text\n",
    "\n",
    "\n",
    "#Summarizer Function   -------------\n",
    "def summarizer(text):\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "    from nltk.stem.snowball import SnowballStemmer\n",
    "    import nltk\n",
    "    import requests\n",
    "    import json\n",
    "    import re\n",
    "\n",
    "    #text=request.json.get('text')   \n",
    "    #text = request.get_json('text')\n",
    "    #text=request.json.get('text')\n",
    "    #text= re.sub(r\"[[0-9]+]\",\"\",text)\n",
    "    #text=re.sub(r'\\([^)]*\\)', '', text)\n",
    "    #text= re.sub(r\"\\[\",\"\",text)\n",
    "\n",
    "    text=str(text) # CONVERTING IS REQUIRED DO NOT REMOVE\n",
    "    \n",
    "    #text=re.replaceAll(\"(\\r?\\n)+\", \"\\n\",text)\n",
    "    #text=re.sub(\"(\\\\n){2,}\", \"\\n\",text)\n",
    "    #print(text)\n",
    "    #================================================\n",
    "    text =re.sub(r\"\\n\\uf0b7 \",\"\\n\",text)\n",
    "    text =re.sub(r'(\\n){2,}', r'\\n', text)\n",
    "    text =re.sub(r'(\\n ){2,}', r'\\n', text)\n",
    "    #================================================ these where added on 4-oct-19 to remove multiple \\n\n",
    "    \n",
    "    text= re.sub(r\"[\\u2022,\\u2023,\\u25E6,\\u2043,\\u2219]\\s\\d\\.\\s[A-z]\",\"\\n\", text)\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stopWords = set(stopwords.words(\"english\"))\n",
    "    words = word_tokenize(text)\n",
    "    freqTable = dict()\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        if word in stopWords:\n",
    "            continue\n",
    "        word = stemmer.stem(word)\n",
    "        if word in freqTable:\n",
    "            freqTable[word] += 1\n",
    "        else:\n",
    "            freqTable[word] = 1\n",
    "\n",
    "    sentences = sent_tokenize(text)\n",
    "    sentenceValue = dict()\n",
    "\n",
    "    for sentence in sentences:\n",
    "        for word, freq in freqTable.items():\n",
    "            if word in sentence.lower():\n",
    "                if sentence in sentenceValue:\n",
    "                    sentenceValue[sentence] += freq\n",
    "                else:\n",
    "                    sentenceValue[sentence] = freq\n",
    "    #print(\"\\n\\n\",sentenceValue,\"\\n\\n\")#=======>\n",
    "    sumValues = 0\n",
    "    for sentence in sentenceValue:\n",
    "        sumValues += sentenceValue[sentence]\n",
    "\n",
    "    # Average value of a sentence from original text\n",
    "    average = int(sumValues / len(sentenceValue))\n",
    "\n",
    "    summary = {}\n",
    "    key=1\n",
    "    for sentence in sentences:\n",
    "        if (sentence in sentenceValue) and (sentenceValue[sentence] > (1.3 * average)):\n",
    "    #print(sentenceValue[sentence],\" with avg= \",1.5 * average)#it was 1.2 initially\n",
    "            summary[key] = sentence\n",
    "            key+=1\n",
    "\n",
    "    clean_10_summary={}\n",
    "    key_num=1\n",
    "    for sent in summary.keys():\n",
    "        if len(summary[sent]) > 20:\n",
    "            clean_10_summary[key_num]=summary[sent]\n",
    "            key_num+=1\n",
    "            #if key_num==21:\n",
    "                #break     \n",
    "        else:\n",
    "            continue\n",
    "    return clean_10_summary\n",
    "\n",
    "   # if __name__=\"apps\":\n",
    "   #     text=open(r\"E:\\stwork\\iso OutPut\\ARTICLE.pdf\")#its a local pdf \n",
    "        #text=pdfconverter().ipynb_checkpoints\n",
    "        \n",
    "        #text=pdfminer(DeprecationWarning) \n",
    "    if __name__ == \"__main__\":\n",
    "        text = pdf_to_text(r'E:\\stwork\\SOME_RANDOM_ARTICLE.pdf')#its a local pdf \n",
    "        #print(text)\n",
    "        summ=summarizer(str(text, 'utf-8'))\n",
    "        #print(str(text, 'utf-8'))\n",
    "        #return summ\n",
    "        for k in summ.keys():\n",
    "            print(\"\\n\",summ[k],\"\\n\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
